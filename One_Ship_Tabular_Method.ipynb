{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halite challenge - basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment import halite_env as Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Environment.halite_env' from '/home/mango/haliteRL/Environment/halite_env.py'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(Env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 1\n",
    "n_actions = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "map_size = 7 # 7 x 7 map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Halite Environment\n"
     ]
    }
   ],
   "source": [
    "env = Env.HaliteEnv(num_players, map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[822, 345, 442, 258, 331, 360, 163],\n",
       "       [498, 369, 595, 936,  66, 996, 837],\n",
       "       [489, 258, 766, 841, 902, 729, 169],\n",
       "       [338, 858,  46,   0, 265, 386,  97],\n",
       "       [522, 440, 425, 343, 828,  31, 625],\n",
       "       [691, 833, 605, 551,   1, 801, 414],\n",
       "       [409, 531, 409, 738, 388, 905,  41]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halite in the map, min = 0, max = 1000\n",
    "env.map[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ship position\n",
    "env.map[:,:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the halite carried from each ship in the position corresponding to the ship\n",
    "# initially there is no ship, hence no halite carried either\n",
    "\n",
    "env.map[:,:,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shipyard position\n",
    "env.map[:,:,3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial halite:  [5000.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial halite: \", env.player_halite[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions are represented as a matrix whose entries are -1 if no ship is in that position, \n",
    "#'a_i' if ship i is present in that position \n",
    "action_matrix = np.full((map_size,map_size), -1) # no ship, no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e81971ffbb87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshipyard_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# initially always choose to create a ship\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# returns the state, i.e. env.map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmakeship\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshipyard_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# s_0 -> map_halite, s_1 -> cargo_halite, s_2 -> shipyard_position (not used), s_3 -> ship_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmap_halite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/haliteRL/Environment/halite_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, makeship, debug)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# cargo arrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mmask_dropoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_coming_ships\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer_halite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shipyard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_dropoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shipyard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_dropoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (1,)"
     ]
    }
   ],
   "source": [
    "# the environment already has in memory the last state, thus we don't need to resubmit it\n",
    "# the only things that we submit are the action matrix and the shipyard action (1 or True to spawn a ship, 0 otherwise)\n",
    "shipyard_action = 1 # initially always choose to create a ship\n",
    "# returns the state, i.e. env.map\n",
    "s, h, finish, _ = env.step(action_matrix, makeship = shipyard_action)\n",
    "# s_0 -> map_halite, s_1 -> cargo_halite, s_2 -> shipyard_position (not used), s_3 -> ship_position\n",
    "map_halite = s[:,:,0]\n",
    "ship_pos_matrix = s[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(v_dec, L):\n",
    "    # v_two = [v1,v2]\n",
    "    # returns the encoded version V[v1,v2] of V = np.arange(0,L)\n",
    "    # L = length(all_possible_v)\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_enc = V[v_dec[0],v_dec[1]] \n",
    "    return v_enc\n",
    "\n",
    "def decode(v_enc, L):\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0]])\n",
    "    return v_dec\n",
    "\n",
    "def one_to_index(V,L):\n",
    "    # matrix V with one entry = 1 and the others 0\n",
    "    return np.arange(L**2).reshape((L, L))[V.astype(bool)]\n",
    "\n",
    "# ship positions x (n_cells + 1) x halite levels -> tensor\n",
    "# we will need a 3D encoding, like the 2D seen above\n",
    "def encode3D(v_dec, L1, L2, L3):\n",
    "    # v_dec = [v1,v2,v3]\n",
    "    # returns the encoded version V[v1,v2,v3] of V = np.arange(0,L1*L2*L3)\n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_enc = V[v_dec[0],v_dec[1], v_dec[2]] \n",
    "    return v_enc\n",
    "\n",
    "def decode3D(v_enc, L1, L2, L3):\n",
    "    # v_enc = V[v1,v2,v3] \n",
    "    # V = np.arange(0,L1*L2*L3)\n",
    "    # returns the decoded version v_dec = [v1,v2,v3] of V[v1,v2,v3] \n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0], np.where(v_enc == V)[2][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded position of the ship:  [24]\n",
      "Decoded position of the ship:  [3 3]\n"
     ]
    }
   ],
   "source": [
    "#position_encoded\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size)\n",
    "print(\"Encoded position of the ship: \", pos_enc)\n",
    "#position_decoded\n",
    "pos_dec = decode(pos_enc, map_size)\n",
    "print(\"Decoded position of the ship: \", pos_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial ship cargo:  0\n"
     ]
    }
   ],
   "source": [
    "ship_cargo = s[pos_dec[0],pos_dec[1],2]\n",
    "print(\"Initial ship cargo: \", ship_cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for the agent\n",
    "def greedy_policy(s, q_values):\n",
    "    return np.argmax(q_values[s])\n",
    "\n",
    "def e_greedy_policy(s, q_values, eps = 0.01):\n",
    "    # s is encoded in input, a is encoded in output\n",
    "    u = np.random.rand()\n",
    "    if u > eps:\n",
    "        return np.argmax(q_values[s])\n",
    "    else:\n",
    "        return np.random.randint(0, len(q_values[s]))\n",
    "    \n",
    "def update_q(s, a, r, sp, ap, q_values, gamma = 1):\n",
    "    q_values[s,a] = r + gamma*q_values[sp,ap]\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State complexity and state approximation\n",
    "\n",
    "We have:\n",
    "- $(map size)^2$ positions ($49$ in this case, up to $64^2 = 4096$);\n",
    "- 1000 values of halite for each position (h_val);\n",
    "- 1000 values of carried halite (h_val).\n",
    "\n",
    "The state of the system is defined by the halite in each cell + the halite carried by the ship + the position of the ship. We can have almost all the possible combinations of the values that those variables can assume, thus we have the combinations of $1000$ values of halite for $50$ cells ($49$ of the map + the one carried by the ship) all multiplied by $49$ possible positions of the ship, for a total of $49\\times 10^{147}$ possible states. If instead we consider the largest map of $64 \\times 64$ we arrive at $4096\\times 10^{12288}$ possible states.\n",
    "\n",
    "WORK IN PROGRESS\n",
    "\n",
    "To tackle this issue we choose to approximate the information about the halite using for example 4 halite levels and the following encoding:\n",
    "- $h = 0$ if $0 \\le halite \\le 1$; \n",
    "- $h = 1$ if $1 < halite \\le 10$; \n",
    "- $h = 2$ if $10 < halite \\le 100$; \n",
    "- $h = 3$ if $100 < halite \\le 1000$.\n",
    "\n",
    "In this way for a $7 \\times 7$ map, the total number of states are $9800$ and for the $64 \\times 64$ there are \"just\" $67.125.248$, that means a lot of training and experience are needed, but we're talking about millions and not tens of billions (an order $10^5$ of difference!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4096*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def halite_quantization(h, q_number = 4):\n",
    "    # h can either be a scalar or a matrix \n",
    "    tresholds = np.logspace(0,3,4) # [1, 10, 100, 1000] = [10^0, 10^1, 10^2, 10^3]\n",
    "    h_shape = h.shape\n",
    "    h_temp = h.flatten()\n",
    "    mask = (h_temp[:,np.newaxis] < tresholds).astype(int)\n",
    "    level = np.argmax(mask, axis = 1)\n",
    "    return level.reshape(h_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use it\n",
    "q_h = halite_quantization(map_halite)\n",
    "c_h = halite_quantization(ship_cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to write a function that is able to get the decoded state from the output of the environment\n",
    "# ship positions x (n_cells + 1) -> ship positions x (n_cells + 1) x halite levels\n",
    "\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size) #this is the first entry\n",
    "\n",
    "# Now we compose an array of the halite values of the n cells and the ship \n",
    "# and encode it through the matrix [(n_cells + 1) x halite levels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_lev = 4 # halite levels\n",
    "n_cells = map_size**2\n",
    "n_states = n_cells*(n_cells+1)*h_lev\n",
    "print(\"Total number of states to be experienced: \", n_states)\n",
    "n_actions = 5\n",
    "tot_turns = 400\n",
    "# to account the fact that the ships can't know the number of turns elapsed we train them as if there was\n",
    "# a uniform probability of 1/tot_turns of ending the game at each turn\n",
    "discount_factor = 1 - 1/tot_turns \n",
    "n_batch = 100 # number of episodes in an epoch\n",
    "epochs = 0\n",
    "q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "max_epochs = 1000\n",
    "reward_score = np.zeros(max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dec = np.array([0,0,1])\n",
    "print(\"Original decoded state: \", s_dec)\n",
    "s_enc = encode3D(s_dec, L1 = n_cells, L2 = n_cells+1, L3 = h_lev)\n",
    "print(\"Encoded state: \", s_enc)\n",
    "s_dec_2 = decode3D(s_enc, L1 = n_cells, L2 = n_cells+1, L3 = h_lev)\n",
    "print(\"New decoded state: \", s_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    reward_progress = np.zeros(n_batch) # bunch of 100 episodes\n",
    "    eps = 0.5 # starting value of epsilon\n",
    "    # generate an adaptive epsilon greedy algorithm, has to be calibrated\n",
    "    epsilons = np.array(list(map(lambda i : eps*np.exp(-i/100), np.arange(0,max_epochs+1))))\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        s_dec = [0,4] # init state: top corners\n",
    "        s_enc = encode(s_dec, 25) # encoded state used to have a scalar index to access the states; 25 singular states\n",
    "        steps = 0\n",
    "        reward = 0\n",
    "        \n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            \n",
    "            a_enc = e_greedy_pol_v1(s_enc, q_values, eps = epsilons[epochs])\n",
    "            a_dec = decode(a_enc, 4) # 4 singular actions\n",
    "            \n",
    "            sp_dec, r = turn_v2(s_dec, a_dec) # interacts with the environment \n",
    "            sp_enc = encode(sp_dec, 25) # encode to scalar state\n",
    "            reward = reward + r\n",
    "            \n",
    "            a_temp_enc = greedy_pol_v0(sp_enc, q_values)\n",
    "            a_temp_dec = decode(a_temp_enc, 4)\n",
    "\n",
    "            q_values = update_q_v0(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = discount_factor)\n",
    "            \n",
    "            # update states\n",
    "            s_enc = sp_enc\n",
    "            s_dec = sp_dec\n",
    "            \n",
    "            \n",
    "            # terminal states are 2 and 24\n",
    "            if (s_dec[0] == 2 and s_dec[1] == 24) or (s_dec[1] == 2 and s_dec[0] == 24):\n",
    "                #print(\"Terminal state reached at step %d.\"%steps)\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "            \"\"\"if (s_dec[0] == s_dec[1]):\n",
    "                #print(\"Agent crushed one on each other.\") \n",
    "                #game_progress[i] = 100 # assign 100 equals to failure (and the -100 reward accounts for the accident)\n",
    "                #break\"\"\"\n",
    "            if steps >= 100:\n",
    "                #print(\"Too much time has passed. Game Over.\")\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "    \n",
    "    print(\"Average reward per episode in epoch %d: \"%epochs, reward_progress.mean())\n",
    "    reward_score[epochs-1] = reward_progress.mean()\n",
    "\n",
    "    if epochs >= max_epochs:\n",
    "        print(\"Hey, I think you've had enough! Let's stop here.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halite challenge - basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment import halite_env as Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Environment.halite_env' from '/home/mango/haliteRL/Environment/halite_env.py'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(Env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 1\n",
    "n_actions = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "map_size = 7 # 7 x 7 map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Halite Environment\n"
     ]
    }
   ],
   "source": [
    "env = Env.HaliteEnv(num_players, map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[822, 345, 442, 258, 331, 360, 163],\n",
       "       [498, 369, 595, 936,  66, 996, 837],\n",
       "       [489, 258, 766, 841, 902, 729, 169],\n",
       "       [338, 858,  46,   0, 265, 386,  97],\n",
       "       [522, 440, 425, 343, 828,  31, 625],\n",
       "       [691, 833, 605, 551,   1, 801, 414],\n",
       "       [409, 531, 409, 738, 388, 905,  41]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halite in the map, min = 0, max = 1000\n",
    "env.map[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ship position\n",
    "env.map[:,:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the halite carried from each ship in the position corresponding to the ship\n",
    "# initially there is no ship, hence no halite carried either\n",
    "\n",
    "env.map[:,:,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shipyard position\n",
    "env.map[:,:,3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial halite:  [5000.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial halite: \", env.player_halite[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions are represented as a matrix whose entries are -1 if no ship is in that position, \n",
    "#'a_i' if ship i is present in that position \n",
    "action_matrix = np.full((map_size,map_size), -1) # no ship, no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e81971ffbb87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshipyard_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# initially always choose to create a ship\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# returns the state, i.e. env.map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmakeship\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshipyard_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# s_0 -> map_halite, s_1 -> cargo_halite, s_2 -> shipyard_position (not used), s_3 -> ship_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmap_halite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/haliteRL/Environment/halite_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, makeship, debug)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# cargo arrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mmask_dropoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_coming_ships\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer_halite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shipyard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_dropoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shipyard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_dropoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (1,)"
     ]
    }
   ],
   "source": [
    "# the environment already has in memory the last state, thus we don't need to resubmit it\n",
    "# the only things that we submit are the action matrix and the shipyard action (1 or True to spawn a ship, 0 otherwise)\n",
    "shipyard_action = 1 # initially always choose to create a ship\n",
    "# returns the state, i.e. env.map\n",
    "s, h, finish, _ = env.step(action_matrix, makeship = shipyard_action)\n",
    "# s_0 -> map_halite, s_1 -> cargo_halite, s_2 -> shipyard_position (not used), s_3 -> ship_position\n",
    "map_halite = s[:,:,0]\n",
    "ship_pos_matrix = s[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(v_dec, L):\n",
    "    # v_two = [v1,v2]\n",
    "    # returns the encoded version V[v1,v2] of V = np.arange(0,L)\n",
    "    # L = length(all_possible_v)\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_enc = V[v_dec[0],v_dec[1]] \n",
    "    return v_enc\n",
    "\n",
    "def decode(v_enc, L):\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0]])\n",
    "    return v_dec\n",
    "\n",
    "def one_to_index(V,L):\n",
    "    # matrix V with one entry = 1 and the others 0\n",
    "    return np.arange(L**2).reshape((L, L))[V.astype(bool)]\n",
    "\n",
    "# ship positions x (n_cells + 1) x halite levels -> tensor\n",
    "# we will need a 3D encoding, like the 2D seen above\n",
    "def encode3D(v_dec, L1, L2, L3):\n",
    "    # v_dec = [v1,v2,v3]\n",
    "    # returns the encoded version V[v1,v2,v3] of V = np.arange(0,L1*L2*L3)\n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_enc = V[v_dec[0],v_dec[1], v_dec[2]] \n",
    "    return v_enc\n",
    "\n",
    "def decode3D(v_enc, L1, L2, L3):\n",
    "    # v_enc = V[v1,v2,v3] \n",
    "    # V = np.arange(0,L1*L2*L3)\n",
    "    # returns the decoded version v_dec = [v1,v2,v3] of V[v1,v2,v3] \n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0], np.where(v_enc == V)[2][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded position of the ship:  [24]\n",
      "Decoded position of the ship:  [3 3]\n"
     ]
    }
   ],
   "source": [
    "#position_encoded\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size)\n",
    "print(\"Encoded position of the ship: \", pos_enc)\n",
    "#position_decoded\n",
    "pos_dec = decode(pos_enc, map_size)\n",
    "print(\"Decoded position of the ship: \", pos_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial ship cargo:  0\n"
     ]
    }
   ],
   "source": [
    "ship_cargo = s[pos_dec[0],pos_dec[1],2]\n",
    "print(\"Initial ship cargo: \", ship_cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for the agent\n",
    "def greedy_policy(s, q_values):\n",
    "    return np.argmax(q_values[s])\n",
    "\n",
    "def e_greedy_policy(s, q_values, eps = 0.01):\n",
    "    # s is encoded in input, a is encoded in output\n",
    "    u = np.random.rand()\n",
    "    if u > eps:\n",
    "        return np.argmax(q_values[s])\n",
    "    else:\n",
    "        return np.random.randint(0, len(q_values[s]))\n",
    "    \n",
    "def update_q(s, a, r, sp, ap, q_values, gamma = 1):\n",
    "    q_values[s,a] = r + gamma*q_values[sp,ap]\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State complexity and state approximation\n",
    "\n",
    "We have:\n",
    "- $(map size)^2$ positions ($49$ in this case, up to $64^2 = 4096$);\n",
    "- 1000 values of halite for each position (h_val);\n",
    "- 1000 values of carried halite (h_val).\n",
    "\n",
    "The state of the system is defined by the halite in each cell + the halite carried by the ship + the position of the ship. We can have almost all the possible combinations of the values that those variables can assume, thus we have the combinations of $1000$ values of halite for $50$ cells ($49$ of the map + the one carried by the ship) all multiplied by $49$ possible positions of the ship, for a total of $49\\times 10^{147}$ possible states. If instead we consider the largest map of $64 \\times 64$ we arrive at $4096\\times 10^{12288}$ possible states.\n",
    "\n",
    "WORK IN PROGRESS\n",
    "\n",
    "To tackle this issue we choose to approximate the information about the halite using for example 4 halite levels and the following encoding:\n",
    "- $h = 0$ if $0 \\le halite \\le 1$; \n",
    "- $h = 1$ if $1 < halite \\le 10$; \n",
    "- $h = 2$ if $10 < halite \\le 100$; \n",
    "- $h = 3$ if $100 < halite \\le 1000$.\n",
    "\n",
    "In this way for a $7 \\times 7$ map, the total number of states are $9800$ and for the $64 \\times 64$ there are \"just\" $67.125.248$, that means a lot of training and experience are needed, but we're talking about millions and not tens of billions (an order $10^5$ of difference!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4096*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def halite_quantization(h, q_number = 4):\n",
    "    # h can either be a scalar or a matrix \n",
    "    tresholds = np.logspace(0,3,4) # [1, 10, 100, 1000] = [10^0, 10^1, 10^2, 10^3]\n",
    "    h_shape = h.shape\n",
    "    h_temp = h.flatten()\n",
    "    mask = (h_temp[:,np.newaxis] < tresholds).astype(int)\n",
    "    level = np.argmax(mask, axis = 1)\n",
    "    return level.reshape(h_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use it\n",
    "q_h = halite_quantization(map_halite)\n",
    "c_h = halite_quantization(ship_cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to write a function that is able to get the decoded state from the output of the environment\n",
    "# ship positions x (n_cells + 1) -> ship positions x (n_cells + 1) x halite levels\n",
    "\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size) #this is the first entry\n",
    "\n",
    "# Now we compose an array of the halite values of the n cells and the ship \n",
    "# and encode it through the matrix [(n_cells + 1) x halite levels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_lev = 4 # halite levels\n",
    "n_cells = map_size**2\n",
    "n_states = n_cells*(n_cells+1)*h_lev\n",
    "print(\"Total number of states to be experienced: \", n_states)\n",
    "n_actions = 5\n",
    "tot_turns = 400\n",
    "# to account the fact that the ships can't know the number of turns elapsed we train them as if there was\n",
    "# a uniform probability of 1/tot_turns of ending the game at each turn\n",
    "discount_factor = 1 - 1/tot_turns \n",
    "n_batch = 100 # number of episodes in an epoch\n",
    "epochs = 0\n",
    "q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "max_epochs = 1000\n",
    "reward_score = np.zeros(max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dec = np.array([0,0,1])\n",
    "print(\"Original decoded state: \", s_dec)\n",
    "s_enc = encode3D(s_dec, L1 = n_cells, L2 = n_cells+1, L3 = h_lev)\n",
    "print(\"Encoded state: \", s_enc)\n",
    "s_dec_2 = decode3D(s_enc, L1 = n_cells, L2 = n_cells+1, L3 = h_lev)\n",
    "print(\"New decoded state: \", s_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    reward_progress = np.zeros(n_batch) # bunch of 100 episodes\n",
    "    eps = 0.5 # starting value of epsilon\n",
    "    # generate an adaptive epsilon greedy algorithm, has to be calibrated\n",
    "    epsilons = np.array(list(map(lambda i : eps*np.exp(-i/100), np.arange(0,max_epochs+1))))\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        s_dec = [0,4] # init state: top corners\n",
    "        s_enc = encode(s_dec, 25) # encoded state used to have a scalar index to access the states; 25 singular states\n",
    "        steps = 0\n",
    "        reward = 0\n",
    "        \n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            \n",
    "            a_enc = e_greedy_pol_v1(s_enc, q_values, eps = epsilons[epochs])\n",
    "            a_dec = decode(a_enc, 4) # 4 singular actions\n",
    "            \n",
    "            sp_dec, r = turn_v2(s_dec, a_dec) # interacts with the environment \n",
    "            sp_enc = encode(sp_dec, 25) # encode to scalar state\n",
    "            reward = reward + r\n",
    "            \n",
    "            a_temp_enc = greedy_pol_v0(sp_enc, q_values)\n",
    "            a_temp_dec = decode(a_temp_enc, 4)\n",
    "\n",
    "            q_values = update_q_v0(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = discount_factor)\n",
    "            \n",
    "            # update states\n",
    "            s_enc = sp_enc\n",
    "            s_dec = sp_dec\n",
    "            \n",
    "            \n",
    "            # terminal states are 2 and 24\n",
    "            if (s_dec[0] == 2 and s_dec[1] == 24) or (s_dec[1] == 2 and s_dec[0] == 24):\n",
    "                #print(\"Terminal state reached at step %d.\"%steps)\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "            \"\"\"if (s_dec[0] == s_dec[1]):\n",
    "                #print(\"Agent crushed one on each other.\") \n",
    "                #game_progress[i] = 100 # assign 100 equals to failure (and the -100 reward accounts for the accident)\n",
    "                #break\"\"\"\n",
    "            if steps >= 100:\n",
    "                #print(\"Too much time has passed. Game Over.\")\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "    \n",
    "    print(\"Average reward per episode in epoch %d: \"%epochs, reward_progress.mean())\n",
    "    reward_score[epochs-1] = reward_progress.mean()\n",
    "\n",
    "    if epochs >= max_epochs:\n",
    "        print(\"Hey, I think you've had enough! Let's stop here.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halite challenge - basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment import halite_env as Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Environment.halite_env' from '/home/mango/haliteRL/Environment/halite_env.py'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(Env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 1\n",
    "n_actions = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "map_size = 7 # 7 x 7 map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Halite Environment\n"
     ]
    }
   ],
   "source": [
    "env = Env.HaliteEnv(num_players, map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[822, 345, 442, 258, 331, 360, 163],\n",
       "       [498, 369, 595, 936,  66, 996, 837],\n",
       "       [489, 258, 766, 841, 902, 729, 169],\n",
       "       [338, 858,  46,   0, 265, 386,  97],\n",
       "       [522, 440, 425, 343, 828,  31, 625],\n",
       "       [691, 833, 605, 551,   1, 801, 414],\n",
       "       [409, 531, 409, 738, 388, 905,  41]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halite in the map, min = 0, max = 1000\n",
    "env.map[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ship position\n",
    "env.map[:,:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the halite carried from each ship in the position corresponding to the ship\n",
    "# initially there is no ship, hence no halite carried either\n",
    "\n",
    "env.map[:,:,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shipyard position\n",
    "env.map[:,:,3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial halite:  [5000.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial halite: \", env.player_halite[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions are represented as a matrix whose entries are -1 if no ship is in that position, \n",
    "#'a_i' if ship i is present in that position \n",
    "action_matrix = np.full((map_size,map_size), -1) # no ship, no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e81971ffbb87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshipyard_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# initially always choose to create a ship\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# returns the state, i.e. env.map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmakeship\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshipyard_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# s_0 -> map_halite, s_1 -> cargo_halite, s_2 -> shipyard_position (not used), s_3 -> ship_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmap_halite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/haliteRL/Environment/halite_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, makeship, debug)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# cargo arrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mmask_dropoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_coming_ships\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer_halite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shipyard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_dropoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shipyard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_dropoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (1,)"
     ]
    }
   ],
   "source": [
    "# the environment already has in memory the last state, thus we don't need to resubmit it\n",
    "# the only things that we submit are the action matrix and the shipyard action (1 or True to spawn a ship, 0 otherwise)\n",
    "shipyard_action = 1 # initially always choose to create a ship\n",
    "# returns the state, i.e. env.map\n",
    "s, h, finish, _ = env.step(action_matrix, makeship = shipyard_action)\n",
    "# s_0 -> map_halite, s_1 -> cargo_halite, s_2 -> shipyard_position (not used), s_3 -> ship_position\n",
    "map_halite = s[:,:,0]\n",
    "ship_pos_matrix = s[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(v_dec, L):\n",
    "    # v_two = [v1,v2]\n",
    "    # returns the encoded version V[v1,v2] of V = np.arange(0,L)\n",
    "    # L = length(all_possible_v)\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_enc = V[v_dec[0],v_dec[1]] \n",
    "    return v_enc\n",
    "\n",
    "def decode(v_enc, L):\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0]])\n",
    "    return v_dec\n",
    "\n",
    "def one_to_index(V,L):\n",
    "    # matrix V with one entry = 1 and the others 0\n",
    "    return np.arange(L**2).reshape((L, L))[V.astype(bool)]\n",
    "\n",
    "# ship positions x (n_cells + 1) x halite levels -> tensor\n",
    "# we will need a 3D encoding, like the 2D seen above\n",
    "def encode3D(v_dec, L1, L2, L3):\n",
    "    # v_dec = [v1,v2,v3]\n",
    "    # returns the encoded version V[v1,v2,v3] of V = np.arange(0,L1*L2*L3)\n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_enc = V[v_dec[0],v_dec[1], v_dec[2]] \n",
    "    return v_enc\n",
    "\n",
    "def decode3D(v_enc, L1, L2, L3):\n",
    "    # v_enc = V[v1,v2,v3] \n",
    "    # V = np.arange(0,L1*L2*L3)\n",
    "    # returns the decoded version v_dec = [v1,v2,v3] of V[v1,v2,v3] \n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0], np.where(v_enc == V)[2][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded position of the ship:  [24]\n",
      "Decoded position of the ship:  [3 3]\n"
     ]
    }
   ],
   "source": [
    "#position_encoded\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size)\n",
    "print(\"Encoded position of the ship: \", pos_enc)\n",
    "#position_decoded\n",
    "pos_dec = decode(pos_enc, map_size)\n",
    "print(\"Decoded position of the ship: \", pos_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial ship cargo:  0\n"
     ]
    }
   ],
   "source": [
    "ship_cargo = s[pos_dec[0],pos_dec[1],2]\n",
    "print(\"Initial ship cargo: \", ship_cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for the agent\n",
    "def greedy_policy(s, q_values):\n",
    "    return np.argmax(q_values[s])\n",
    "\n",
    "def e_greedy_policy(s, q_values, eps = 0.01):\n",
    "    # s is encoded in input, a is encoded in output\n",
    "    u = np.random.rand()\n",
    "    if u > eps:\n",
    "        return np.argmax(q_values[s])\n",
    "    else:\n",
    "        return np.random.randint(0, len(q_values[s]))\n",
    "    \n",
    "def update_q(s, a, r, sp, ap, q_values, gamma = 1):\n",
    "    q_values[s,a] = r + gamma*q_values[sp,ap]\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State complexity and state approximation\n",
    "\n",
    "We have:\n",
    "- $(map size)^2$ positions ($49$ in this case, up to $64^2 = 4096$);\n",
    "- 1000 values of halite for each position (h_val);\n",
    "- 1000 values of carried halite (h_val).\n",
    "\n",
    "The state of the system is defined by the halite in each cell + the halite carried by the ship + the position of the ship. We can have almost all the possible combinations of the values that those variables can assume, thus we have the combinations of $1000$ values of halite for $50$ cells ($49$ of the map + the one carried by the ship) all multiplied by $49$ possible positions of the ship, for a total of $49\\times 10^{147}$ possible states. If instead we consider the largest map of $64 \\times 64$ we arrive at $4096\\times 10^{12288}$ possible states.\n",
    "\n",
    "WORK IN PROGRESS\n",
    "\n",
    "To tackle this issue we choose to approximate the information about the halite using for example 4 halite levels and the following encoding:\n",
    "- $h = 0$ if $0 \\le halite \\le 1$; \n",
    "- $h = 1$ if $1 < halite \\le 10$; \n",
    "- $h = 2$ if $10 < halite \\le 100$; \n",
    "- $h = 3$ if $100 < halite \\le 1000$.\n",
    "\n",
    "In this way for a $7 \\times 7$ map, the total number of states are $9800$ and for the $64 \\times 64$ there are \"just\" $67.125.248$, that means a lot of training and experience are needed, but we're talking about millions and not tens of billions (an order $10^5$ of difference!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4096*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def halite_quantization(h, q_number = 4):\n",
    "    # h can either be a scalar or a matrix \n",
    "    tresholds = np.logspace(0,3,4) # [1, 10, 100, 1000] = [10^0, 10^1, 10^2, 10^3]\n",
    "    h_shape = h.shape\n",
    "    h_temp = h.flatten()\n",
    "    mask = (h_temp[:,np.newaxis] < tresholds).astype(int)\n",
    "    level = np.argmax(mask, axis = 1)\n",
    "    return level.reshape(h_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use it\n",
    "q_h = halite_quantization(map_halite)\n",
    "c_h = halite_quantization(ship_cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to write a function that is able to get the decoded state from the output of the environment\n",
    "# ship positions x (n_cells + 1) -> ship positions x (n_cells + 1) x halite levels\n",
    "\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size) #this is the first entry\n",
    "\n",
    "# Now we compose an array of the halite values of the n cells and the ship \n",
    "# and encode it through the matrix [(n_cells + 1) x halite levels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_lev = 4 # halite levels\n",
    "n_cells = map_size**2\n",
    "n_states = n_cells*(n_cells+1)*h_lev\n",
    "print(\"Total number of states to be experienced: \", n_states)\n",
    "n_actions = 5\n",
    "tot_turns = 400\n",
    "# to account the fact that the ships can't know the number of turns elapsed we train them as if there was\n",
    "# a uniform probability of 1/tot_turns of ending the game at each turn\n",
    "discount_factor = 1 - 1/tot_turns \n",
    "n_batch = 100 # number of episodes in an epoch\n",
    "epochs = 0\n",
    "q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "max_epochs = 1000\n",
    "reward_score = np.zeros(max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dec = np.array([0,0,1])\n",
    "print(\"Original decoded state: \", s_dec)\n",
    "s_enc = encode3D(s_dec, L1 = n_cells, L2 = n_cells+1, L3 = h_lev)\n",
    "print(\"Encoded state: \", s_enc)\n",
    "s_dec_2 = decode3D(s_enc, L1 = n_cells, L2 = n_cells+1, L3 = h_lev)\n",
    "print(\"New decoded state: \", s_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    reward_progress = np.zeros(n_batch) # bunch of 100 episodes\n",
    "    eps = 0.5 # starting value of epsilon\n",
    "    # generate an adaptive epsilon greedy algorithm, has to be calibrated\n",
    "    epsilons = np.array(list(map(lambda i : eps*np.exp(-i/100), np.arange(0,max_epochs+1))))\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        s_dec = [0,4] # init state: top corners\n",
    "        s_enc = encode(s_dec, 25) # encoded state used to have a scalar index to access the states; 25 singular states\n",
    "        steps = 0\n",
    "        reward = 0\n",
    "        \n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            \n",
    "            a_enc = e_greedy_pol_v1(s_enc, q_values, eps = epsilons[epochs])\n",
    "            a_dec = decode(a_enc, 4) # 4 singular actions\n",
    "            \n",
    "            sp_dec, r = turn_v2(s_dec, a_dec) # interacts with the environment \n",
    "            sp_enc = encode(sp_dec, 25) # encode to scalar state\n",
    "            reward = reward + r\n",
    "            \n",
    "            a_temp_enc = greedy_pol_v0(sp_enc, q_values)\n",
    "            a_temp_dec = decode(a_temp_enc, 4)\n",
    "\n",
    "            q_values = update_q_v0(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = discount_factor)\n",
    "            \n",
    "            # update states\n",
    "            s_enc = sp_enc\n",
    "            s_dec = sp_dec\n",
    "            \n",
    "            \n",
    "            # terminal states are 2 and 24\n",
    "            if (s_dec[0] == 2 and s_dec[1] == 24) or (s_dec[1] == 2 and s_dec[0] == 24):\n",
    "                #print(\"Terminal state reached at step %d.\"%steps)\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "            \"\"\"if (s_dec[0] == s_dec[1]):\n",
    "                #print(\"Agent crushed one on each other.\") \n",
    "                #game_progress[i] = 100 # assign 100 equals to failure (and the -100 reward accounts for the accident)\n",
    "                #break\"\"\"\n",
    "            if steps >= 100:\n",
    "                #print(\"Too much time has passed. Game Over.\")\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "    \n",
    "    print(\"Average reward per episode in epoch %d: \"%epochs, reward_progress.mean())\n",
    "    reward_score[epochs-1] = reward_progress.mean()\n",
    "\n",
    "    if epochs >= max_epochs:\n",
    "        print(\"Hey, I think you've had enough! Let's stop here.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halite challenge - basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment import halite_env as Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Environment.halite_env' from '/home/mango/haliteRL/Environment/halite_env.py'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(Env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 1\n",
    "n_actions = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "map_size = 7 # 7 x 7 map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Halite Environment\n"
     ]
    }
   ],
   "source": [
    "env = Env.HaliteEnv(num_players, map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[822, 345, 442, 258, 331, 360, 163],\n",
       "       [498, 369, 595, 936,  66, 996, 837],\n",
       "       [489, 258, 766, 841, 902, 729, 169],\n",
       "       [338, 858,  46,   0, 265, 386,  97],\n",
       "       [522, 440, 425, 343, 828,  31, 625],\n",
       "       [691, 833, 605, 551,   1, 801, 414],\n",
       "       [409, 531, 409, 738, 388, 905,  41]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halite in the map, min = 0, max = 1000\n",
    "env.map[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ship position\n",
    "env.map[:,:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the halite carried from each ship in the position corresponding to the ship\n",
    "# initially there is no ship, hence no halite carried either\n",
    "\n",
    "env.map[:,:,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shipyard position\n",
    "env.map[:,:,3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial halite:  [5000.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial halite: \", env.player_halite[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions are represented as a matrix whose entries are -1 if no ship is in that position, \n",
    "#'a_i' if ship i is present in that position \n",
    "action_matrix = np.full((map_size,map_size), -1) # no ship, no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e81971ffbb87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshipyard_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# initially always choose to create a ship\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# returns the state, i.e. env.map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmakeship\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshipyard_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# s_0 -> map_halite, s_1 -> cargo_halite, s_2 -> shipyard_position (not used), s_3 -> ship_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmap_halite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/haliteRL/Environment/halite_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, makeship, debug)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# cargo arrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mmask_dropoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_coming_ships\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer_halite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shipyard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_dropoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_shipyard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_dropoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (0,) could not be broadcast to indexing result of shape (1,)"
     ]
    }
   ],
   "source": [
    "# the environment already has in memory the last state, thus we don't need to resubmit it\n",
    "# the only things that we submit are the action matrix and the shipyard action (1 or True to spawn a ship, 0 otherwise)\n",
    "shipyard_action = 1 # initially always choose to create a ship\n",
    "# returns the state, i.e. env.map\n",
    "s, h, finish, _ = env.step(action_matrix, makeship = shipyard_action)\n",
    "# s_0 -> map_halite, s_1 -> cargo_halite, s_2 -> shipyard_position (not used), s_3 -> ship_position\n",
    "map_halite = s[:,:,0]\n",
    "ship_pos_matrix = s[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(v_dec, L):\n",
    "    # v_two = [v1,v2]\n",
    "    # returns the encoded version V[v1,v2] of V = np.arange(0,L)\n",
    "    # L = length(all_possible_v)\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_enc = V[v_dec[0],v_dec[1]] \n",
    "    return v_enc\n",
    "\n",
    "def decode(v_enc, L):\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0]])\n",
    "    return v_dec\n",
    "\n",
    "def one_to_index(V,L):\n",
    "    # matrix V with one entry = 1 and the others 0\n",
    "    return np.arange(L**2).reshape((L, L))[V.astype(bool)]\n",
    "\n",
    "# ship positions x (n_cells + 1) x halite levels -> tensor\n",
    "# we will need a 3D encoding, like the 2D seen above\n",
    "def encode3D(v_dec, L1, L2, L3):\n",
    "    # v_dec = [v1,v2,v3]\n",
    "    # returns the encoded version V[v1,v2,v3] of V = np.arange(0,L1*L2*L3)\n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_enc = V[v_dec[0],v_dec[1], v_dec[2]] \n",
    "    return v_enc\n",
    "\n",
    "def decode3D(v_enc, L1, L2, L3):\n",
    "    # v_enc = V[v1,v2,v3] \n",
    "    # V = np.arange(0,L1*L2*L3)\n",
    "    # returns the decoded version v_dec = [v1,v2,v3] of V[v1,v2,v3] \n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0], np.where(v_enc == V)[2][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded position of the ship:  [24]\n",
      "Decoded position of the ship:  [3 3]\n"
     ]
    }
   ],
   "source": [
    "#position_encoded\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size)\n",
    "print(\"Encoded position of the ship: \", pos_enc)\n",
    "#position_decoded\n",
    "pos_dec = decode(pos_enc, map_size)\n",
    "print(\"Decoded position of the ship: \", pos_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial ship cargo:  0\n"
     ]
    }
   ],
   "source": [
    "ship_cargo = s[pos_dec[0],pos_dec[1],2]\n",
    "print(\"Initial ship cargo: \", ship_cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for the agent\n",
    "def greedy_policy(s, q_values):\n",
    "    return np.argmax(q_values[s])\n",
    "\n",
    "def e_greedy_policy(s, q_values, eps = 0.01):\n",
    "    # s is encoded in input, a is encoded in output\n",
    "    u = np.random.rand()\n",
    "    if u > eps:\n",
    "        return np.argmax(q_values[s])\n",
    "    else:\n",
    "        return np.random.randint(0, len(q_values[s]))\n",
    "    \n",
    "def update_q(s, a, r, sp, ap, q_values, gamma = 1):\n",
    "    q_values[s,a] = r + gamma*q_values[sp,ap]\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State complexity and state approximation\n",
    "\n",
    "We have:\n",
    "- $(map size)^2$ positions ($49$ in this case, up to $64^2 = 4096$);\n",
    "- 1000 values of halite for each position (h_val);\n",
    "- 1000 values of carried halite (h_val).\n",
    "\n",
    "The state of the system is defined by the halite in each cell + the halite carried by the ship + the position of the ship. We can have almost all the possible combinations of the values that those variables can assume, thus we have the combinations of $1000$ values of halite for $50$ cells ($49$ of the map + the one carried by the ship) all multiplied by $49$ possible positions of the ship, for a total of $49\\times 10^{147}$ possible states. If instead we consider the largest map of $64 \\times 64$ we arrive at $4096\\times 10^{12288}$ possible states.\n",
    "\n",
    "WORK IN PROGRESS\n",
    "\n",
    "To tackle this issue we choose to approximate the information about the halite using for example 4 halite levels and the following encoding:\n",
    "- $h = 0$ if $0 \\le halite \\le 1$; \n",
    "- $h = 1$ if $1 < halite \\le 10$; \n",
    "- $h = 2$ if $10 < halite \\le 100$; \n",
    "- $h = 3$ if $100 < halite \\le 1000$.\n",
    "\n",
    "In this way for a $7 \\times 7$ map, the total number of states are $9800$ and for the $64 \\times 64$ there are \"just\" $67.125.248$, that means a lot of training and experience are needed, but we're talking about millions and not tens of billions (an order $10^5$ of difference!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4096*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def halite_quantization(h, q_number = 4):\n",
    "    # h can either be a scalar or a matrix \n",
    "    tresholds = np.logspace(0,3,4) # [1, 10, 100, 1000] = [10^0, 10^1, 10^2, 10^3]\n",
    "    h_shape = h.shape\n",
    "    h_temp = h.flatten()\n",
    "    mask = (h_temp[:,np.newaxis] < tresholds).astype(int)\n",
    "    level = np.argmax(mask, axis = 1)\n",
    "    return level.reshape(h_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use it\n",
    "q_h = halite_quantization(map_halite)\n",
    "c_h = halite_quantization(ship_cargo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to write a function that is able to get the decoded state from the output of the environment\n",
    "# ship positions x (n_cells + 1) -> ship positions x (n_cells + 1) x halite levels\n",
    "\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size) #this is the first entry\n",
    "\n",
    "# Now we compose an array of the halite values of the n cells and the ship \n",
    "# and encode it through the matrix [(n_cells + 1) x halite levels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_lev = 4 # halite levels\n",
    "n_cells = map_size**2\n",
    "n_states = n_cells*(n_cells+1)*h_lev\n",
    "print(\"Total number of states to be experienced: \", n_states)\n",
    "n_actions = 5\n",
    "tot_turns = 400\n",
    "# to account the fact that the ships can't know the number of turns elapsed we train them as if there was\n",
    "# a uniform probability of 1/tot_turns of ending the game at each turn\n",
    "discount_factor = 1 - 1/tot_turns \n",
    "n_batch = 100 # number of episodes in an epoch\n",
    "epochs = 0\n",
    "q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "max_epochs = 1000\n",
    "reward_score = np.zeros(max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dec = np.array([0,0,1])\n",
    "print(\"Original decoded state: \", s_dec)\n",
    "s_enc = encode3D(s_dec, L1 = n_cells, L2 = n_cells+1, L3 = h_lev)\n",
    "print(\"Encoded state: \", s_enc)\n",
    "s_dec_2 = decode3D(s_enc, L1 = n_cells, L2 = n_cells+1, L3 = h_lev)\n",
    "print(\"New decoded state: \", s_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    reward_progress = np.zeros(n_batch) # bunch of 100 episodes\n",
    "    eps = 0.5 # starting value of epsilon\n",
    "    # generate an adaptive epsilon greedy algorithm, has to be calibrated\n",
    "    epsilons = np.array(list(map(lambda i : eps*np.exp(-i/100), np.arange(0,max_epochs+1))))\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        s_dec = [0,4] # init state: top corners\n",
    "        s_enc = encode(s_dec, 25) # encoded state used to have a scalar index to access the states; 25 singular states\n",
    "        steps = 0\n",
    "        reward = 0\n",
    "        \n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            \n",
    "            a_enc = e_greedy_pol_v1(s_enc, q_values, eps = epsilons[epochs])\n",
    "            a_dec = decode(a_enc, 4) # 4 singular actions\n",
    "            \n",
    "            sp_dec, r = turn_v2(s_dec, a_dec) # interacts with the environment \n",
    "            sp_enc = encode(sp_dec, 25) # encode to scalar state\n",
    "            reward = reward + r\n",
    "            \n",
    "            a_temp_enc = greedy_pol_v0(sp_enc, q_values)\n",
    "            a_temp_dec = decode(a_temp_enc, 4)\n",
    "\n",
    "            q_values = update_q_v0(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = discount_factor)\n",
    "            \n",
    "            # update states\n",
    "            s_enc = sp_enc\n",
    "            s_dec = sp_dec\n",
    "            \n",
    "            \n",
    "            # terminal states are 2 and 24\n",
    "            if (s_dec[0] == 2 and s_dec[1] == 24) or (s_dec[1] == 2 and s_dec[0] == 24):\n",
    "                #print(\"Terminal state reached at step %d.\"%steps)\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "            \"\"\"if (s_dec[0] == s_dec[1]):\n",
    "                #print(\"Agent crushed one on each other.\") \n",
    "                #game_progress[i] = 100 # assign 100 equals to failure (and the -100 reward accounts for the accident)\n",
    "                #break\"\"\"\n",
    "            if steps >= 100:\n",
    "                #print(\"Too much time has passed. Game Over.\")\n",
    "                reward_progress[i] = reward\n",
    "                game_progress[i] = steps\n",
    "                terminal_states[:,i] = s_dec\n",
    "                break\n",
    "    \n",
    "    print(\"Average reward per episode in epoch %d: \"%epochs, reward_progress.mean())\n",
    "    reward_score[epochs-1] = reward_progress.mean()\n",
    "\n",
    "    if epochs >= max_epochs:\n",
    "        print(\"Hey, I think you've had enough! Let's stop here.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
