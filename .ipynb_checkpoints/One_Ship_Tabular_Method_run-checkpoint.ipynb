{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halite challenge - basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment import halite_env as Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Environment.halite_env' from '/home/nicola/Nicola_unipd/QuartoAnno/TODO/Baiesi/RL/haliteRL/Environment/halite_env.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(Env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the environment - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 1\n",
    "map_size = 7 # 7 x 7 map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env.HaliteEnv(num_players, map_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[261, 860, 535, 691, 711, 735, 154],\n",
       "       [343, 669, 193, 333, 777, 639, 664],\n",
       "       [943, 437, 708, 743, 289, 420, 463],\n",
       "       [850,  36,  23,   0, 775, 614, 814],\n",
       "       [539, 280, 368, 643, 848, 690, 371],\n",
       "       [141, 152, 944,  13,  86, 927, 443],\n",
       "       [447, 766, 673, 796, 543, 758, 501]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halite in the map, min = 0, max = 1000\n",
    "env.map[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ship position\n",
    "env.map[:,:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the halite carried from each ship in the position corresponding to the ship\n",
    "# initially there is no ship, hence no halite carried either\n",
    "env.map[:,:,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shipyard position\n",
    "env.map[:,:,3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial halite:  [5000.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial halite: \", env.player_halite[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000.]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# actions are represented as a matrix whose entries are -1 if no ship is in that position, \n",
    "#'a_i' if ship i is present in that position \n",
    "action_matrix = np.full((map_size,map_size), -1) # no ship, no action\n",
    "\n",
    "# the environment already has in memory the last state, thus we don't need to resubmit it\n",
    "# the only things that we submit are the action matrix and the shipyard action (1 or True to spawn a ship, 0 otherwise)\n",
    "shipyard_action = 1 # initially always choose to create a ship\n",
    "# returns the state, i.e. env.map\n",
    "s, h, finish, _ = env.step(action_matrix, makeship = shipyard_action)\n",
    "print(h[0])\n",
    "print(finish)\n",
    "# s_0 -> map_halite, s_1 -> ship_position, s_2 -> cargo_halite, s_3 -> shipyard_position (not used)\n",
    "map_halite = s[:,:,0]\n",
    "ship_pos_matrix = s[:,:,1]\n",
    "shipy_pos_matrix = s[:,:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State complexity and state approximation\n",
    "\n",
    "We have:\n",
    "- $(map\\_size)^2$ positions ($49$ in this case, up to $64^2 = 4096$ for the largest map);\n",
    "- $1000$ values of halite for each position ($h\\_lev$);\n",
    "- $1000$ values of carried halite ($h\\_lev$).\n",
    "\n",
    "The state of the system is defined by the position of the ship + the halite in EACH cell + the halite carried by the ship. We can have almost all the possible combinations of the values that those variables can assume, thus we have the combinations of $1000$ values of halite for $50$ cells ($49$ of the map + the one carried by the ship) all multiplied by $49$ possible positions of the ship, for a total of $49\\times 10^{147}$ possible states. If instead we consider the largest map of $64 \\times 64$ we arrive at $4096\\times 10^{12288}$ possible states.\n",
    "\n",
    "The general formula can be written as: \n",
    "$$\\# states = (map\\_size)^2 \\times (h\\_lev)^{map\\_size^2+1}$$\n",
    "\n",
    "where we consider the map always centered on the shipyard, hence in this framework its position is fixed and not considered as a variable.\n",
    "Of course this state representation is totally uncontrollable, because it scales exponentially in the number of cells of the map.\n",
    "\n",
    "To tackle this issue we choose to drastically reduce the amount of information that is observed by the ship through two processes: partial observation and state abstraction.\n",
    "\n",
    "\n",
    "\n",
    "### Partial observation: depth of field\n",
    "\n",
    "The most expensive dependence in the formula about the number of states of the system is the exponent at which is elevated $h\\_lev$. This is obtained considering all possible combinations of halite for all the cells of the map and the halite carried by the ship. A different approach is to consider only the halite inside the field of view of the agent and restrict the depth of field to the minimal possible quantity, i.e. nearest neighbors. In this way, independently from the $map\\_size$ we get an exponent that in 2D is equal to 6 (4 for the neighbors, 1 for the state in which the ship stands and 1 for the halite it carries).\n",
    "\n",
    "In other words, we get: \n",
    "\n",
    "$$\\# states = (map\\_size)^2 \\times (h\\_lev)^{6}$$\n",
    "\n",
    "that yields $4.9 \\times 10^{19}$ for the $7 \\times 7$ map, that is still not manageable, but considerably smaller ( of order $ \\approx 10^{120}$).\n",
    "\n",
    "### State abstraction: halite quantization\n",
    "\n",
    "Differently from the restriction on the observation space, that are somewhat straightforward, the state abstraction must involve some hypothesis about the environment that involve knowing the model of the environment. For example, if we were in the situation on not knowing how the halite is collected we probably would have done a different choice.\n",
    "Since we know that the ship collects an amount of halite proportional to the halite in the cell (25% of it, to be more exact) and pays a fee of 10% of the halite contained in a cell to leave it, we are more interested in having encoded the notions of \"low\" and \"high\" halite levels, instead of sampling with precision the middle-high half of the halite scale. \n",
    "To be more specific, we choose to approximate the information about the halite using for $h_lev = 3$ halite levels and the following encoding:\n",
    "- $h = 0$ if $halite \\le 10$; \n",
    "- $h = 1$ if $10 < halite \\le 100$; \n",
    "- $h = 2$ if $100 < halite \\le 1000$.\n",
    "\n",
    "The important part is that the halite is quantized in intervals that grow of a decade each, but we could also test adding a fourth level.\n",
    "In this way the number of states of the system becomes:\n",
    "\n",
    "$$\\# states = (map\\_size)^2 \\times 3^{6}$$\n",
    "\n",
    "yielding for a $7 \\times 7$ map $35.721$ states, that is reachable with our resources. \n",
    "\n",
    "### State abstraction: meta-informations\n",
    "\n",
    "The problem of these manipulations is that now the ship has access only to local informations and lacks of the knowledge about the position of the shipyard (but again, being the latter fixed, only the ship position is needed) and of that of distant halite deposits. In order to enhance the ability of the ship to find those deposits, we encode in a 4-states additional information the direction that the ship should take to go towards the nearest and richest deposit.\n",
    "In this final formulation, the total number of states that needs to be experienced by the ship is:\n",
    "\n",
    "$$\\# states = (map\\_size)^2 \\times 3^{6} * 4$$\n",
    "\n",
    "yielding for a $7 \\times 7$ map the final result of $142.884$ states. Considering that each of these states requires 64 bits, i.e. 8 bytes, to be stored, the memory required to store the Q-value table is 1.143.072 bytes = 1.143 Mb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State encoding\n",
    "\n",
    "We have seen how many states we have to handle for a $7 \\times 7$ map, but each state is composed by different informations:\n",
    "\n",
    "- $pos\\_enc \\in [0,48]$ (ship position encoded);\n",
    "- $halite\\_vector = (C, O, S, N, E, W)$ halite (where C stands for the halite carried by the ship and O for the cell occupied by the ship );\n",
    "- $halite\\_direction \\in [0,3]$ (action to take to go towards the nearest and richest halite deposit).\n",
    "\n",
    "The idea is to first encode the vector of halite using a one hot encoding, then to form a 3D tensor of $pos\\_enc \\times halvec\\_enc \\times haldir$ and encode it in a 1D array. This final array $s\\_enc$ will form the rows of the Q(s,a) table and should assume values between 0 and 142.883."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize the environment\n",
    "env = Env.HaliteEnv(num_players, map_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_matrix = np.full((map_size,map_size), -1) # no ship, no action\n",
    "shipyard_action = 1 # initially always choose to create a ship\n",
    "# returns the state, i.e. env.map\n",
    "s, h, finish, _ = env.step(action_matrix, makeship = shipyard_action)\n",
    "# s_0 -> map_halite, s_1 -> ship_position, s_2 -> cargo_halite, s_3 -> shipyard_position (not used)\n",
    "map_halite = s[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ship position (encoded and decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix to scalar encoding\n",
    "\n",
    "def one_to_index(V,L):\n",
    "    # matrix V with one entry = 1 and the others 0\n",
    "    return np.arange(L**2).reshape((L, L))[V.astype(bool)]\n",
    "\n",
    "# 2D encoding and decoding\n",
    "\n",
    "def encode(v_dec, L):\n",
    "    # v_two = [v1,v2]\n",
    "    # returns the encoded version V[v1,v2] of V = np.arange(0,L)\n",
    "    # L = length(all_possible_v)\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_enc = V[v_dec[0],v_dec[1]] \n",
    "    return v_enc\n",
    "\n",
    "def decode(v_enc, L):\n",
    "    V = np.arange(0,L**2).reshape((L,L))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded position of the ship:  [24]\n",
      "Decoded position of the ship:  [3 3]\n",
      "Encoded position of the shipyard:  [24]\n",
      "Decoded position of the shipyard:  [3 3]\n"
     ]
    }
   ],
   "source": [
    "ship_pos_matrix = s[:,:,1]\n",
    "shipy_pos_matrix = s[:,:,3]\n",
    "\n",
    "#position_encoded of the ship\n",
    "pos_enc = one_to_index(ship_pos_matrix, map_size)\n",
    "print(\"Encoded position of the ship: \", pos_enc)\n",
    "#position_decoded of the ship\n",
    "pos_dec = decode(pos_enc, map_size)\n",
    "print(\"Decoded position of the ship: \", pos_dec)\n",
    "\n",
    "#position_encoded of the ship\n",
    "shipy_enc = one_to_index(shipy_pos_matrix, map_size)\n",
    "print(\"Encoded position of the shipyard: \", shipy_enc)\n",
    "#position_decoded of the ship\n",
    "shipy_dec = decode(shipy_enc, map_size)\n",
    "print(\"Decoded position of the shipyard: \", shipy_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting halite vector (encoded and decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halite_vec_dec(state, q_number = 3, map_size = 7):\n",
    "    \n",
    "    def halite_quantization(h, q_number = 3):\n",
    "        # h can either be a scalar or a matrix \n",
    "        tresholds = np.logspace(1,3,q_number) # [10, 100, 1000] = [10^1, 10^2, 10^3]\n",
    "        h_shape = h.shape\n",
    "        h_temp = h.flatten()\n",
    "        mask = (h_temp[:,np.newaxis] < tresholds).astype(int)\n",
    "        level = np.argmax(mask, axis = 1)\n",
    "        return level.reshape(h_shape)\n",
    "\n",
    "    pos_enc = one_to_index(state[:,:,1], map_size)\n",
    "    pos_dec = decode(pos_enc, map_size) # decode position to access matrix by two indices\n",
    "    \n",
    "    ship_cargo = state[pos_dec[0],pos_dec[1],2]\n",
    "    #print(\"Halite carried: \", ship_cargo)\n",
    "    cargo_quant = halite_quantization(ship_cargo).reshape(1)[0] # quantize halite\n",
    "    \n",
    "    map_halite = state[:,:,0]\n",
    "    halite_quant = halite_quantization(map_halite) # quantize halite\n",
    "    \n",
    "    halite_vector = []\n",
    "    halite_vector.append(cargo_quant)\n",
    "    halite_vector.append(halite_quant[pos_dec[0], pos_dec[1]])\n",
    "    halite_vector.append(halite_quant[(pos_dec[0]+1)%map_size, pos_dec[1]])\n",
    "    halite_vector.append(halite_quant[(pos_dec[0]-1)%map_size, pos_dec[1]])\n",
    "    halite_vector.append(halite_quant[pos_dec[0], (pos_dec[1]+1)%map_size])\n",
    "    halite_vector.append(halite_quant[pos_dec[0], (pos_dec[1]-1)%map_size])\n",
    "    #print(\"Quantized halite vector: \", halite_vector)\n",
    "    return np.array(halite_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "halvec_dec = get_halite_vec_dec(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to encode this vector of 6 elements whose values can be 0,1 or 2 in a scalar between 0 and $3^6 - 1$ (the encoding, of course, must be unique). The idea is similar to that of the 2D case, where we have a matrix whose element are numbered from 0 to the total number of elements - 1:\n",
    "- to get the encoded state we read the element value whose indices are given by the ordered values stored in the array;\n",
    "- to decode the encoded state and recover the decompressed array, we create a tensor (before it was a matrix) with the same shape of the tensor (matrix) that contains all the numbered states; this tensor has all the entries equal to the scalar value of the encoded state and we compare the two tensor and take the indices of the only element that is equal to the encoded state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid for encoding and decoding an array of length L whose entries can all assume only \n",
    "# the same integer values from 0 to m\n",
    "def encode_tensor(v_dec, L = 6, m = 3):\n",
    "    T = np.arange(m**L).reshape(tuple([m for i in range(L)]))\n",
    "    return T[tuple(v_dec)]\n",
    "\n",
    "def decode_tensor(v_enc, L = 6, m = 3):\n",
    "    T = np.arange(m**L).reshape(tuple([m for i in range(L)]))\n",
    "    return np.array([np.where(v_enc == T)[i][0] for i in range(L)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector:  [0 0 2 2 2 2]\n",
      "Encoded vector:  80\n",
      "Decoded vector:  [0 0 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original vector: \", halvec_dec)\n",
    "v_enc = encode_tensor(halvec_dec)\n",
    "print(\"Encoded vector: \", v_enc)\n",
    "v_dec = decode_tensor(v_enc)\n",
    "print(\"Decoded vector: \", v_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting halite direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_halite_direction(state, map_size = 7):\n",
    "    \n",
    "    def roll_and_cut(M, shift, axis, border = 1, center = (3,3)):\n",
    "        M_temp = np.roll(M, shift = shift, axis = axis)\n",
    "        M_cut = M_temp[center[0]-border:center[0]+border+1, center[1]-border:center[1]+border+1]\n",
    "        return M_cut\n",
    "\n",
    "    map_halite = state[:,:,0] # matrix with halite of each cell of the map\n",
    "    \n",
    "    pos_enc = one_to_index(state[:,:,1], map_size) # ship position\n",
    "    pos_dec = decode(pos_enc, map_size) # decode position to access matrix by two indices\n",
    "    \n",
    "    shipy_enc = one_to_index(shipy_pos_matrix, map_size) # shipyard position\n",
    "    shipy_dec = decode(shipy_enc, map_size) #position_decoded \n",
    "    \n",
    "    shift = (shipy_dec[0]-pos_dec[0],shipy_dec[1]-pos_dec[1])\n",
    "    centered_h = np.roll(map_halite, shift = shift, axis = (0,1)) #centers map_halite on the ship\n",
    "    \n",
    "    mean_cardinal_h = []\n",
    "    # this could be generalized to wider areas, like 5x5, but 3x3 it's enough for a 7x7 map\n",
    "    perm = [(a,sh) for a in [0,1] for sh in [-2,2]] # permutations of shifts and axis to get the 4 cardinal directions\n",
    "    for a,sh in perm:\n",
    "        mean_h = np.mean(roll_and_cut(centered_h, shift = sh, axis = a), axis = (0,1))\n",
    "        mean_cardinal_h.append(mean_h)\n",
    "\n",
    "    mean_cardinal_h = np.array(mean_cardinal_h)\n",
    "    halite_direction = np.argmax(mean_cardinal_h) #+ 1 # take the direction of the 3x3 most rich zone\n",
    "    \n",
    "    return halite_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what we should get: \n",
      " [[515 552  87 370 653   2 798]\n",
      " [346 101 745 960   6 504 208]\n",
      " [797 329  60 520 717  86 575]\n",
      " [194 577 573 724 215 704 937]\n",
      " [855 548 646 229   0 373 445]\n",
      " [665 659  76 279 272 293 765]\n",
      " [692  96 158 829 152  56 393]]\n",
      "Result: \n",
      " [[515 552  87 370 653   2 798]\n",
      " [346 101 745 960   6 504 208]\n",
      " [797 329  60 520 717  86 575]\n",
      " [194 577 573 724 215 704 937]\n",
      " [855 548 646 229   0 373 445]\n",
      " [665 659  76 279 272 293 765]\n",
      " [692  96 158 829 152  56 393]]\n",
      "Neighborhood of the ship: \n",
      " [[ 60 520 717]\n",
      " [573 724 215]\n",
      " [646 229   0]] \n",
      "\n",
      "Map shifted in direction: (-2,0)\n",
      " [[646 229   0]\n",
      " [ 76 279 272]\n",
      " [158 829 152]]\n",
      "Mean halite in direction: (-2,0) 293.44444444444446 \n",
      "\n",
      "Map shifted in direction: (2,0)\n",
      " [[ 87 370 653]\n",
      " [745 960   6]\n",
      " [ 60 520 717]]\n",
      "Mean halite in direction: (2,0) 457.55555555555554 \n",
      "\n",
      "Map shifted in direction: (-2,1)\n",
      " [[717  86 575]\n",
      " [215 704 937]\n",
      " [  0 373 445]]\n",
      "Mean halite in direction: (-2,1) 450.22222222222223 \n",
      "\n",
      "Map shifted in direction: (2,1)\n",
      " [[797 329  60]\n",
      " [194 577 573]\n",
      " [855 548 646]]\n",
      "Mean halite in direction: (2,1) 508.77777777777777 \n",
      "\n",
      "Action suggested to reach the nearest and richest halite deposit:  3\n"
     ]
    }
   ],
   "source": [
    "# Explanatory cell\n",
    "\n",
    "# now suppose that the ship is in [2,2], whereas the shipyard is at the center of the map, i.e. [3,3]\n",
    "example = np.roll(map_halite, shift = (1,1) , axis =  (0,1)) #in this way we simulate the ship to be in (2,2)\n",
    "print(\"This is what we should get: \\n\", example)\n",
    "pos_dec = [2,2]\n",
    "shift = (shipy_dec[0]-pos_dec[0],shipy_dec[1]-pos_dec[1])\n",
    "centered_h = np.roll(map_halite, shift = shift, axis = (0,1))\n",
    "print(\"Result: \\n\",centered_h)\n",
    "\n",
    "def roll_and_cut_v0(M, shift, axis, border = 1, center = (3,3)):\n",
    "        M_temp = np.roll(M, shift = shift, axis = axis)\n",
    "        M_cut = M_temp[center[0]-border:center[0]+border+1, center[1]-border:center[1]+border+1]\n",
    "        return M_cut\n",
    "    \n",
    "# try to return just the 3x3 area around the ship\n",
    "around_ship = roll_and_cut_v0(centered_h, shift = 0, axis = 0)\n",
    "print(\"Neighborhood of the ship: \\n\", around_ship, '\\n')\n",
    "# we actually need to do this shifting by two in all cardinal directions w.r.t. the map centered around the ship\n",
    "mean_cardinal_h = []\n",
    "perm = [(a,sh) for a in [0,1] for sh in [-2,2]]\n",
    "for a,sh in perm:\n",
    "    print(\"Map shifted in direction: (%d,%d)\\n\"%(sh,a), roll_and_cut_v0(centered_h, shift = sh, axis = a))\n",
    "    mean_h = np.mean(roll_and_cut_v0(centered_h, shift = sh, axis = a), axis = (0,1))\n",
    "    print(\"Mean halite in direction: (%d,%d)\"%(sh,a), mean_h, '\\n')\n",
    "    mean_cardinal_h.append(mean_h)\n",
    "\n",
    "mean_cardinal_h = np.array(mean_cardinal_h)\n",
    "halite_direction = np.argmax(mean_cardinal_h) #+ 1\n",
    "print(\"Action suggested to reach the nearest and richest halite deposit: \", halite_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and decoding the total state\n",
    "\n",
    "Starting from the encoded position, the encoded halite vector and the halite direction, we finally encode and decode this 3 contributions in a unique scalar that will take values between 0 and 142.883."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D encoding and decoding for arbitrary lengths of the three axis\n",
    "\n",
    "def encode3D(v_dec, L1, L2, L3):\n",
    "    # v_dec = [v1,v2,v3]\n",
    "    # returns the encoded version V[v1,v2,v3] of V = np.arange(0,L1*L2*L3)\n",
    "    #print(\"v_dec: \", v_dec)\n",
    "    #print(\"L1 = %d, L2 = %d, L3 = %d\"%(L1,L2,L3))\n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_enc = V[tuple(v_dec)] \n",
    "    return v_enc\n",
    "\n",
    "def decode3D(v_enc, L1, L2, L3):\n",
    "    # v_enc = V[v1,v2,v3] \n",
    "    # V = np.arange(0,L1*L2*L3)\n",
    "    # returns the decoded version v_dec = [v1,v2,v3] of V[v1,v2,v3] \n",
    "    V = np.arange(0,L1*L2*L3).reshape((L1,L2,L3))\n",
    "    v_dec = np.array([np.where(v_enc == V)[0][0],np.where(v_enc == V)[1][0], np.where(v_enc == V)[2][0]])\n",
    "    return v_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "80\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(pos_enc[0])\n",
    "halvec_enc = encode_tensor(halvec_dec)\n",
    "print(halvec_enc)\n",
    "print(halite_direction)\n",
    "s_dec = np.array([pos_enc[0], halvec_enc, halite_direction] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of states to be experienced:  142884\n",
      "Original decoded state:  [24 80  3]\n",
      "Encoded state:  70307\n",
      "New decoded state:  [24 80  3]\n"
     ]
    }
   ],
   "source": [
    "h_lev = 3 # halite levels\n",
    "n_cells = map_size**2 # number of cells in a square map\n",
    "n_states = n_cells*h_lev**6*4\n",
    "n_actions = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "print(\"Total number of states to be experienced: \", n_states)\n",
    "\n",
    "print(\"Original decoded state: \", s_dec)\n",
    "s_enc = encode3D(s_dec, L1 = n_cells, L2 = h_lev**6, L3 = n_actions-1)\n",
    "print(\"Encoded state: \", s_enc)\n",
    "s_dec_2 = decode3D(s_enc, L1 = n_cells, L2 = h_lev**6, L3 = n_actions-1)\n",
    "print(\"New decoded state: \", s_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put all together in order to have a single function that takes the multi-layer state output by the environment and returns the encoded state (that is the one we will use to access the Q-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(state, map_size = 7, h_lev = h_lev, n_actions = n_actions, debug = False):\n",
    "    \n",
    "    pos_enc = one_to_index(state[:,:,1], map_size)[0] # ship position\n",
    "    if debug:\n",
    "        print(\"Ship position encoded in [0,%d]: \"%(map_size**2-1), pos_enc)\n",
    "    \n",
    "    halvec_dec = get_halite_vec_dec(state, q_number = 3, map_size = map_size) \n",
    "    halvec_enc = encode_tensor(halvec_dec) # halite vector\n",
    "    if debug:\n",
    "        print(\"Halite vector encoded in [0,%d]: \"%(h_lev**6 -1), halvec_enc)\n",
    "    \n",
    "    haldir = get_halite_direction(state, map_size = map_size) # halite direction\n",
    "    if debug:\n",
    "        print(\"Halite direction in [1,4]: \", haldir)\n",
    "    \n",
    "    s_dec = np.array([pos_enc, halvec_enc, haldir])\n",
    "    if debug:\n",
    "        print(\"Decoded state: \", s_dec)\n",
    "    s_enc = encode3D(s_dec, L1 = map_size**2, L2 = h_lev**6, L3 = n_actions-1)\n",
    "    if debug:\n",
    "        print(\"State encoded in [0, %d]: \"%(map_size**2*h_lev**6*(n_actions-1)), s_enc, '\\n')\n",
    "    \n",
    "    return s_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70306"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_state(s, map_size = 7, h_lev = h_lev, n_actions = n_actions, debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar and matricial action\n",
    "\n",
    "The environment works for a generic number of ships. For this reason the action submitted must be a matrix of $map\\_size \\times map\\_size$ filled with -1 except for the entry of the ship position, where it has to be entered the value of the action choosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_to_matrix_action(action, state, map_size = 7):\n",
    "    # first get the decoded position of the ship\n",
    "    ship_pos_matrix = state[:,:,1]\n",
    "    pos_enc = one_to_index(ship_pos_matrix, map_size)\n",
    "    pos_dec = decode(pos_enc, map_size)\n",
    "    # then fill a matrix of -1\n",
    "    mat_action = np.full((map_size,map_size), -1)\n",
    "    # finally insert the action in the pos_dec entry\n",
    "    mat_action[tuple(pos_dec)] = action\n",
    "    return mat_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@\n",
    "# RL agent functions\n",
    "#@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "def greedy_policy(s, q_values):\n",
    "    return np.argmax(q_values[s])\n",
    "\n",
    "def e_greedy_policy(s, q_values, eps = 0.01):\n",
    "    # s is encoded in input, a is encoded in output\n",
    "    u = np.random.rand()\n",
    "    if u > eps:\n",
    "        return np.argmax(q_values[s])\n",
    "    else:\n",
    "        return np.random.randint(0, len(q_values[s]))\n",
    "\n",
    "    \n",
    "def update_q_v0(s, a, r, sp, ap, q_values, gamma = 1):\n",
    "    q_values[s,a] = r + gamma*q_values[sp,ap]\n",
    "    return q_values\n",
    "\n",
    "def update_q_v1(s, a, r, sp, ap, q_values, gamma = 1, n_cells = 49, h_lev = 3, n_actions = 5, alpha = 0.1):\n",
    "    s_dec = decode3D(s, L1 = n_cells, L2 = h_lev**6, L3 = n_actions-1)\n",
    "    sp_dec = decode3D(sp, L1 = n_cells, L2 = h_lev**6, L3 = n_actions-1)\n",
    "    shipy_pos = (n_cells-1)/2 #shipyard is at the center of the map\n",
    "    if (sp_dec[0] == shipy_pos and s_dec[0] != shipy_pos):\n",
    "        q_values[s,a] = r # sp is terminal state -> enforce to have Q-value = 0 for all actions ap\n",
    "        #print(\"Terminal value update rule executed.\")\n",
    "    else:\n",
    "        q_values[s,a] = (1-alpha)*q_values[s,a] + alpha*(r + gamma*q_values[sp,ap]) # normal update\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of states to be experienced:  142884\n"
     ]
    }
   ],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@\n",
    "# Environment variables\n",
    "#@@@@@@@@@@@@@@@@@@@@@@\n",
    "NUM_PLAYERS = 1\n",
    "MAP_SIZE = 7 # 7 x 7 map\n",
    "TOT_TURNS = 400 # number of turns for each episode\n",
    "\n",
    "#@@@@@@@@@@@@@@@@\n",
    "# State variables\n",
    "#@@@@@@@@@@@@@@@@\n",
    "H_LEV = 3 # halite levels\n",
    "N_CELLS = MAP_SIZE**2 # number of cells in a square map\n",
    "N_STATES = N_CELLS*H_LEV**6*4\n",
    "N_ACTIONS = 5 # no dropoffs, 1 action for staying still, 4 for moving in the cardinal directions\n",
    "print(\"Total number of states to be experienced: \", N_STATES)\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@\n",
    "# Learning parameters\n",
    "#@@@@@@@@@@@@@@@@@@@@\n",
    "N_BATCH = 20 #100 # number of episodes in an epoch\n",
    "MAX_EPOCHS = 300 # max number of epochs played before stopping (500 ~ 7.3 hours of training)\n",
    "DISCOUNT_FACTOR = 1 - 1/TOT_TURNS #train ships as if each turn has a probability of 1/tot_turns of ending the game \n",
    "STD_REWARD = -0.01\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e2aba37809477680c234b1baac716d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode in epoch 1: -1.722\n",
      "Average halite collected per episode in epoch 1: 2268.150\n",
      "Average reward per episode in epoch 2: -1.696\n",
      "Average halite collected per episode in epoch 2: 2294.000\n"
     ]
    }
   ],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@\n",
    "# Learning variables\n",
    "#@@@@@@@@@@@@@@@@@@@\n",
    "#q_values = np.zeros((n_states,n_actions)) #initialize to zero\n",
    "q_values = np.load(\"Q_values.npy\") # or re-use the one already trained\n",
    "reward_score = np.zeros(MAX_EPOCHS)\n",
    "halite_score = np.zeros(MAX_EPOCHS)\n",
    "epochs = 0\n",
    "\n",
    "from tqdm import tnrange\n",
    "\n",
    "for k in tnrange(MAX_EPOCHS):\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    # here starts an epoch\n",
    "    #@@@@@@@@@@@@@@@@@@@@@@\n",
    "    epochs = epochs + 1\n",
    "    reward_progress = np.zeros(N_BATCH) # bunch of 100 episodes\n",
    "    halite_progress = np.zeros(N_BATCH) # bunch of 100 episodes\n",
    "    eps = 0.5 # starting value of epsilon\n",
    "    # generate an adaptive epsilon greedy algorithm, calibrated in order to have epsilon = 10^-4 at the last epoch\n",
    "    epsilons = np.array(list(map(lambda i : eps*np.exp(-i*2*np.log(10)/MAX_EPOCHS), np.arange(0,MAX_EPOCHS+1))))\n",
    "    \n",
    "    for i in range(N_BATCH):\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        # here starts an episode\n",
    "        #@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        env = Env.HaliteEnv(NUM_PLAYERS, MAP_SIZE, episode_lenght = TOT_TURNS) # init environment\n",
    "        steps = 0\n",
    "        reward = 0 # cumulative reward of the episode\n",
    "        \n",
    "        # first mandatory step\n",
    "        steps = steps + 1\n",
    "        #print(\"\\nStep number %d:\"%steps)\n",
    "        action_matrix = np.full((MAP_SIZE,MAP_SIZE), -1) # no ship, no action\n",
    "        shipyard_action = 1 # initially always choose to create a ship\n",
    "        # returns the matricial state, the array of players halite and a flag that is true if it's the final turn\n",
    "        state, players_halite, finish, _ = env.step(action_matrix, makeship = shipyard_action) \n",
    "        #print(\"Cargo layer: \\n\", state[:,:,2])\n",
    "        current_halite = players_halite[0][0]\n",
    "        s_enc = encode_state(state, map_size = MAP_SIZE, h_lev = H_LEV, n_actions = N_ACTIONS, debug=False)\n",
    "        \n",
    "        while True:\n",
    "            steps = steps + 1\n",
    "            #print(\"\\nStep number %d:\"%steps)\n",
    "            #print(\"Current halite: \", current_halite)\n",
    "            a_enc = e_greedy_policy(s_enc, q_values, eps = epsilons[epochs])\n",
    "            a_mat = scalar_to_matrix_action(a_enc, state, map_size = MAP_SIZE)\n",
    "            \n",
    "            # submit the action and get the new state\n",
    "            state, players_halite, finish, _ = env.step(a_mat, makeship = False) \n",
    "            #print(\"Cargo layer: \\n\", state[:,:,2])\n",
    "            new_halite = players_halite[0][0]\n",
    "            #print(\"New halite: \", new_halite)\n",
    "            # compute the 1--ship reward as the halite increment of the player divided by the max halite \n",
    "            # plus a standard negative reward \n",
    "            r = (new_halite - current_halite)/1000 + STD_REWARD\n",
    "            #print(\"Reward obtained: \", r)\n",
    "            sp_enc = encode_state(state, map_size = MAP_SIZE, h_lev = H_LEV, n_actions = N_ACTIONS, debug=False)\n",
    "            reward = reward + r # cumulative reward of the episode, obsolete\n",
    "            \n",
    "            a_temp_enc = greedy_policy(sp_enc, q_values) # simulate the best action in the new state (before update)\n",
    "            \n",
    "            # update Q-values\n",
    "            #q_values = update_q_v0(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = discount_factor)\n",
    "            q_values = update_q_v1(s_enc, a_enc, r, sp_enc, a_temp_enc, q_values, gamma = DISCOUNT_FACTOR, \n",
    "                                n_cells = N_CELLS, h_lev = H_LEV, n_actions = N_ACTIONS, alpha = LEARNING_RATE)\n",
    "            # update states and halite\n",
    "            s_enc = sp_enc\n",
    "            current_halite = new_halite\n",
    "            \n",
    "            if (finish == True) or (steps >= 400):\n",
    "                #print(\"End episode.\")\n",
    "                reward_progress[i] = reward\n",
    "                halite_progress[i] = current_halite - 4000\n",
    "                break\n",
    "        # play just 1 episode\n",
    "                \n",
    "    # play just 1 epoch\n",
    "\n",
    "    print(\"Average reward per episode in epoch %d: %.3f\"%(epochs, reward_progress.mean()))\n",
    "    print(\"Average halite collected per episode in epoch %d: %.3f\"%(epochs,halite_progress.mean()))\n",
    "    reward_score[epochs-1] = reward_progress.mean()\n",
    "    halite_score[epochs-1] = halite_progress.mean()\n",
    "\n",
    "    if epochs >= MAX_EPOCHS:\n",
    "        print(\"Hey, I think you've had enough! Let's stop here.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Q_values\", q_values)\n",
    "#Q_values = np.load(\"Q_values.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_score[:epochs-1], label = 'MDP')\n",
    "plt.xlabel(\"epochs\", fontsize = 14)\n",
    "plt.ylabel(\"reward\", fontsize = 14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(halite_score[:epochs-1], label = 'MDP')\n",
    "plt.xlabel(\"epochs\", fontsize = 14)\n",
    "plt.ylabel(\"halite collected\", fontsize = 14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = 100*400*max_epochs\n",
    "tot_q = q_values.shape[0]* q_values.shape[1]\n",
    "print(\"Maximal amount of states-actions experienced: \", experience)\n",
    "print(\"Total amount of states-actions to be experienced: \", tot_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug\n",
    "\n",
    "Problem (hypothesis): terminal state coincide with the start state, that has a lower value. Thus becomes inconvinient to deposit the halite.\n",
    "Solution: if the position of the ship is the one of the shipyard and the previous position is different from that, Use Q_max(s') = 0 to update. In this way we should enforce the terminal state to be of value zero and consequently the state from which we arrive will have value r (?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.arange(n_cells).reshape((map_size,map_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_max = np.argmax(q_values, axis=1) #choose for each state the action whose value is the max\n",
    "a_shape = a_max.reshape((n_cells,h_lev**6,n_actions-1)) #shape this vector in a decoded way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_shipy_left = a_shape[23,:,:] # select for example the position at the left of the shipyard\n",
    "# now reshape again so that we can highlight the variable of the \n",
    "# halite carried (the first dimension)\n",
    "a_tens_left = a_shipy_left.reshape((3,3,3,3,3,3,n_actions-1)) \n",
    "# select all the possible configurations in which the ship has the max level of halite carried\n",
    "# and make an histogram of the favourite choices\n",
    "# (1,2,3,4) = (S,N,E,W)\n",
    "# notice that to get to the shipyard and deposit the halite it should choose to go to the east (3), \n",
    "# but this is actually the least choosen option!\n",
    "plt.hist(a_tens_left[2,...].flatten()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we look at the right\n",
    "a_shipy_right = a_shape[25,:,:]\n",
    "a_tens_right = a_shipy_right.reshape((3,3,3,3,3,3,n_actions-1)) \n",
    "plt.hist(a_tens_right[2,...].flatten()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we look up\n",
    "a_shipy_right = a_shape[17,:,:]\n",
    "a_tens_right = a_shipy_right.reshape((3,3,3,3,3,3,n_actions-1)) \n",
    "plt.hist(a_tens_right[2,...].flatten()) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we look up\n",
    "a_shipy_right = a_shape[31,:,:]\n",
    "a_tens_right = a_shipy_right.reshape((3,3,3,3,3,3,n_actions-1)) \n",
    "plt.hist(a_tens_right[2,...].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
